{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Yang\n",
    "# CS 525 Natural Language Processing\n",
    "# Assignment 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk import pos_tag_sents, pos_tag\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "import torch\n",
    "!pip install transformers\n",
    "from transformers import pipeline\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBTAIN THE REVIEWS DATASET\n",
    "reviews_df = pd.read_csv('Reviews.csv')\n",
    "print(\"The size of the dataset:\", reviews_df.shape)\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4589390",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    df.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time', 'Summary'], axis=1, inplace=True)\n",
    "\n",
    "    df['Label'] = df['Score'].map(lambda x: 1 if x > 3 else 0)\n",
    "    df.drop('Score', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def imbalance_resampling(df, num):\n",
    "    # RESAMPLE THE DATA TO MITIGATE IMBALANCE\n",
    "    df_majority = df[df['Label'] == 1] # Filters minorities and keeps majority class labels\n",
    "    df_minority = df[df['Label'] == 0] # Filters majorities and keeps minority class labels\n",
    "\n",
    "    # Upsample minority class\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                  replace=True,     # sample with replacement\n",
    "                                  n_samples=int(num/2))    # to match majority class\n",
    "\n",
    "    # Undersample majority class\n",
    "    df_majority_undersampled = resample(df_majority, \n",
    "                                  replace=True,     # sample with replacement\n",
    "                                  n_samples=int(num/2))    # to match majority class\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    reviews_df_resampled = pd.concat([df_majority_undersampled, df_minority_upsampled])\n",
    "    reviews_df_resampled = reviews_df_resampled.reset_index(drop=True)\n",
    "\n",
    "    return reviews_df_resampled\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1950a9a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews_df_prep = data_preprocessing(reviews_df)\n",
    "reviews_df_resampled = imbalance_resampling(reviews_df_prep, 50000)\n",
    "\n",
    "reviews_df_resampled['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1365e",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM TF-IDF ANALYSIS OF DATASET\n",
    "y = reviews_df_resampled['Label']\n",
    "X = reviews_df_resampled['Text']\n",
    "\n",
    "# PERFORM 70-30 SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "print(\"Train data:\",  X_train.shape, y_train.shape)\n",
    "print(\"Test data:\",  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676bb7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print('X_train_review_tfidf shape: ', X_train_tfidf.shape)\n",
    "print('X_test_review_tfidf shape: ', X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f416d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RANDOM FOREST CLASSIFIER\n",
    "random_forest_clf = RandomForestClassifier(max_depth=100, min_samples_leaf=2, verbose=2)\n",
    "random_forest_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = random_forest_clf.predict(X_test_tfidf)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp\n",
    "p = tp/(tp+fp)\n",
    "r = tp/(tp+fn)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", p)\n",
    "print(\"Recall Score:\", r)\n",
    "print(\"F1 Score:\", (2*p*r)/(p+r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6dea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADABOOST CLASSIFIER\n",
    "adaboost_clf = AdaBoostClassifier(n_estimators=100)\n",
    "adaboost_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = adaboost_clf.predict(X_test_tfidf)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp\n",
    "p = tp/(tp+fp)\n",
    "r = tp/(tp+fn)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", p)\n",
    "print(\"Recall Score:\", r)\n",
    "print(\"F1 Score:\", (2*p*r)/(p+r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISION TREE CLASSIFIER\n",
    "dt_clf = DecisionTreeClassifier(max_depth=100, min_samples_leaf=2)\n",
    "dt_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = dt_clf.predict(X_test_tfidf)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp\n",
    "p = tp/(tp+fp)\n",
    "r = tp/(tp+fn)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", p)\n",
    "print(\"Recall Score:\", r)\n",
    "print(\"F1 Score:\", (2*p*r)/(p+r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a81ff",
   "metadata": {},
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575520da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE WORD2VEC EMBEDDINGS\n",
    "embeddings = gensim_api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c176a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CONVERT EACH DOCUMENT TO THE LATENT REPRESENTATION\n",
    "try:  \n",
    "    curr_num = 1\n",
    "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "    stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
    "    for doc in reviews_df_resampled['Text'].str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
    "        temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "        for word in doc.split(' '): # looping through each word of a single document and spliting through space\n",
    "            if word not in stopwords: # if word is not present in stopwords then (try)\n",
    "                try:\n",
    "                    word_vec = embeddings[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                    temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "                except:\n",
    "                    pass\n",
    "        doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)\n",
    "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "        curr_num += 1\n",
    "\n",
    "        if curr_num == 5000:\n",
    "            print(\"10% Done\")\n",
    "        elif curr_num == 10000:\n",
    "            print(\"20% Done\")\n",
    "        elif curr_num == 15000:\n",
    "            print(\"30% Done\")\n",
    "        elif curr_num == 20000:\n",
    "            print(\"40% Done\")\n",
    "        elif curr_num == 25000:\n",
    "            print(\"50% Done\")\n",
    "        elif curr_num == 30000:\n",
    "            print(\"60% Done\")\n",
    "        elif curr_num == 35000:\n",
    "            print(\"70% Done\")\n",
    "        elif curr_num == 40000:\n",
    "            print(\"80% Done\")\n",
    "        elif curr_num == 45000:\n",
    "            print(\"90% Done\")\n",
    "        elif curr_num == 50000:\n",
    "            print(\"100% Done\")\n",
    "    print(\"Successful Processing,\", docs_vectors.shape)\n",
    "except:\n",
    "    print(\"This either didn't work, or took too long that I got annoyed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further processing\n",
    "docs_vectors = docs_vectors.reset_index(drop=True)\n",
    "reviews_df_resampled = reviews_df_resampled.reset_index(drop=True)\n",
    "docs_vectors['Label'] = reviews_df_resampled['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f00e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE DOCS_VECTOR DF TO LOCAL DRIVE\n",
    "# Uncomment line below to save model\n",
    "# docs_vectors.to_csv('docs_vectors_embedding.csv', index = False)\n",
    "\n",
    "# LOAD THE DOCS_VECTOR DF FROM LOCAL DRIVE\n",
    "# Uncomment line below to load saved model\n",
    "docs_vectors = pd.read_csv('docs_vectors_embedding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a931ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels\n",
    "y = docs_vectors['Label']\n",
    "X = docs_vectors.drop('Label', axis=1)\n",
    "\n",
    "# PERFORM 70-30 SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "print(\"Train data:\",  X_train.shape, y_train.shape)\n",
    "print(\"Test data:\",  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fba9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RANDOM FOREST CLASSIFIER\n",
    "random_forest_clf = RandomForestClassifier(max_depth=100, min_samples_leaf=2, verbose=2)\n",
    "random_forest_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest_clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp\n",
    "p = tp/(tp+fp)\n",
    "r = tp/(tp+fn)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", p)\n",
    "print(\"Recall Score:\", r)\n",
    "print(\"F1 Score:\", (2*p*r)/(p+r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d99379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADABOOST CLASSIFIER\n",
    "adaboost_clf = AdaBoostClassifier(n_estimators=100)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = adaboost_clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp\n",
    "p = tp/(tp+fp)\n",
    "r = tp/(tp+fn)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", p)\n",
    "print(\"Recall Score:\", r)\n",
    "print(\"F1 Score:\", (2*p*r)/(p+r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISION TREE CLASSIFIER\n",
    "dt_clf = DecisionTreeClassifier(max_depth=100, min_samples_leaf=2)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp\n",
    "p = tp/(tp+fp)\n",
    "r = tp/(tp+fn)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", p)\n",
    "print(\"Recall Score:\", r)\n",
    "print(\"F1 Score:\", (2*p*r)/(p+r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3daab",
   "metadata": {},
   "source": [
    "# TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THE PRETRAINED BERT SENTIMENT CLASSIFIER\n",
    "bert_clf = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the text to limit of 512 characters\n",
    "MAX_LENGTH = 512\n",
    "reviews_df_resampled['Text_trunc'] = reviews_df_resampled['Text'].str.slice(0,MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09272712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM SENTIMENT CLASSIFICATION ON EACH REVIEW\n",
    "try:\n",
    "    bert_results = bert_clf(reviews_df_resampled['Text_trunc'].tolist())\n",
    "    bert_results = pd.DataFrame(bert_results)\n",
    "\n",
    "except:\n",
    "    print(\"This either didn't work, or took too long that I got annoyed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54103d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results['true_labels'] = reviews_df_resampled['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT 'POSITIVE' AND 'NEGATIVE' LABELS TO 1 AND 0 RESPECTIVELY\n",
    "label_mapping = {'POSITIVE': 1, 'NEGATIVE': 0}\n",
    "bert_results['label'] = bert_results['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE BERT CLASSIFICATION RESULTS TO LOCAL DRIVE\n",
    "# Uncomment line below to save model\n",
    "# bert_results.to_csv('bert_pretrained_results.csv', index = False)\n",
    "\n",
    "# LOAD THE BERT CLASSIFICATION RESULTS FROM LOCAL DRIVE\n",
    "# Uncomment line below to load saved model\n",
    "bert_results = pd.read_csv('bert_pretrained_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETERMINE PERFORMANCE METRICS OF THE PRETRAINED BERT MODEL\n",
    "y_pred = bert_results['label']\n",
    "y_true = bert_results['true_labels']\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp\n",
    "p = tp/(tp+fp)\n",
    "r = tp/(tp+fn)\n",
    "print('Test Accuracy: ', accuracy_score(y_true, y_pred))\n",
    "print(\"Precision Score:\", p)\n",
    "print(\"Recall Score:\", r)\n",
    "print(\"F1 Score:\", (2*p*r)/(p+r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95e09a",
   "metadata": {},
   "source": [
    "# TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we will train: base uncased BERT\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06564e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR SAKE OF SIMPLICITY, USE ANOTHER RESAMPLE FROM SAME REVIEW DATASET\n",
    "bert_finetuning_df = imbalance_resampling(reviews_df_prep, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93108f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM 70-30 SPLIT\n",
    "texts = bert_finetuning_df['Text']\n",
    "labels = bert_finetuning_df['Label']\n",
    "test_size = 0.3\n",
    "train_texts, valid_texts, train_labels, valid_labels = train_test_split(texts, labels, test_size=test_size)\n",
    "train_texts = train_texts.reset_index(drop=True)\n",
    "valid_texts = valid_texts.reset_index(drop=True)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "valid_labels = valid_labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b1f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512 # character limit for BERT model\n",
    "# Tokenize the dataset, truncate when passed `max_length`, and pad with 0's when less than `max_length`\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "valid_encodings = tokenizer(valid_texts.tolist(), truncation=True, padding=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewGroupsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = ReviewGroupsDataset(train_encodings, train_labels)\n",
    "valid_dataset = ReviewGroupsDataset(valid_encodings, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the BERT classification model\n",
    "bert_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating performance metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE TRAINING ARGUMENTS INSTANCE\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "    logging_steps=200,               # log & save weights each logging_steps\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE TRAINER INSTANCE\n",
    "trainer = Trainer(\n",
    "    model=bert_model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e4038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PERFORM FINE-TUNING OF THE BERT MODEL\n",
    "try:\n",
    "    # train the model\n",
    "    trainer.train()\n",
    "except:\n",
    "    print(\"This either didn't work, or took too long that I got annoyed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path where the fine-tuned model will be saved\n",
    "model_path = 'sentiment-analysis-bert-base-uncased'\n",
    "\n",
    "# SAVE THE FINE-TUNED BERT MODEL AND TOKENIZER TO LOCAL DRIVE\n",
    "# Uncomment the two lines below to save model and tokenizer\n",
    "# bert_model.save_pretrained(model_path)\n",
    "# tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# LOAD THE FINE-TUNED BERT MODEL AND TOKENIZER FROM LOCAL DRIVE\n",
    "# Uncomment the two lines below to save model and tokenizer\n",
    "bert_model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25850972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment prediction\n",
    "MAX_LENGTH = 512\n",
    "def get_prediction(text, convert_to_label=False):\n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "    # perform inference to our model\n",
    "    outputs = bert_model(**inputs)\n",
    "    # get output probabilities by doing softmax\n",
    "    probs = outputs[0].softmax(1)\n",
    "    # executing argmax function to get the candidate label\n",
    "    d = {\n",
    "        0: \"negative\",\n",
    "        1: \"positive\"\n",
    "    }\n",
    "    if convert_to_label:\n",
    "        return d[int(probs.argmax())]\n",
    "    else:\n",
    "        return int(probs.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b630f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PERFORM SENTIMENT CLASSIFICATION ON EACH REVIEW IN THE DATASET\n",
    "bert_finetuning_df = imbalance_resampling(reviews_df_prep, 15000)\n",
    "\n",
    "try:\n",
    "    curr_num = 1\n",
    "    bert_predictions = []\n",
    "    for doc in bert_finetuning_df['Text']:\n",
    "        prediction = get_prediction(doc, convert_to_label=False)\n",
    "        bert_predictions.append(prediction)\n",
    "        curr_num += 1\n",
    "\n",
    "        if curr_num == 750:\n",
    "            print(\"5% Done\")\n",
    "        if curr_num == 1500:\n",
    "            print(\"10% Done\")\n",
    "        if curr_num == 2250:\n",
    "            print(\"15% Done\")\n",
    "        elif curr_num == 3000:\n",
    "            print(\"20% Done\")\n",
    "        elif curr_num == 3750:\n",
    "            print(\"25% Done\")            \n",
    "        elif curr_num == 4500:\n",
    "            print(\"30% Done\")\n",
    "        elif curr_num == 5250:\n",
    "            print(\"35% Done\")\n",
    "        elif curr_num == 6000:\n",
    "            print(\"40% Done\")\n",
    "        elif curr_num == 6750:\n",
    "            print(\"45% Done\")\n",
    "        elif curr_num == 7500:\n",
    "            print(\"50% Done\")\n",
    "        elif curr_num == 8250:\n",
    "            print(\"55% Done\")        \n",
    "        elif curr_num == 9000:\n",
    "            print(\"60% Done\")\n",
    "        elif curr_num == 9750:\n",
    "            print(\"65% Done\")\n",
    "        elif curr_num == 10500:\n",
    "            print(\"70% Done\")\n",
    "        elif curr_num == 11250:\n",
    "            print(\"75% Done\")\n",
    "        elif curr_num == 12000:\n",
    "            print(\"80% Done\")\n",
    "        elif curr_num == 12750:\n",
    "            print(\"85% Done\")\n",
    "        elif curr_num == 13500:\n",
    "            print(\"90% Done\")\n",
    "        elif curr_num == 14250:\n",
    "            print(\"95% Done\")\n",
    "        elif curr_num == 15000:\n",
    "            print(\"100% Done\")\n",
    "    print(\"Successful Processing\")\n",
    "\n",
    "except:\n",
    "    print(\"This either didn't work, or took too long that I got annoyed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b956f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predictions = pd.DataFrame(bert_predictions, columns = ['prediction'])\n",
    "bert_predictions['truth'] = bert_finetuning_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETERMINE PERFORMANCE METRICS OF THE FINE-TUNED BERT MODEL\n",
    "y_pred = bert_predictions['truth']\n",
    "y_true = bert_predictions['prediction']\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp\n",
    "p = tp/(tp+fp)\n",
    "r = tp/(tp+fn)\n",
    "print('Test Accuracy: ', accuracy_score(y_true, y_pred))\n",
    "print(\"Precision Score:\", p)\n",
    "print(\"Recall Score:\", r)\n",
    "print(\"F1 Score:\", (2*p*r)/(p+r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e90d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
